{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hMVnwRDQOadNFZDwac8PbeDxMFXrWGju",
      "authorship_tag": "ABX9TyNCw4BuAqoaVGi/NJsWfujN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaikrishnaParamkusham/NLP_GEN_AI/blob/master/Assignment_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the sentiment analysis data (test.csv) Data: https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset?select=test.csv\n",
        "\n",
        "2. You have to perform some preprocessing\n",
        "\n",
        "3. There should be two columns (text, sentiment)\n",
        "\n",
        "4. Then perform text encoding or embedding over the text data and label encoding on target column\n",
        "\n",
        "5. Write a RNN code in PyTorch from scratch to train sentiment analysis model on batch data using Dataset and Dataloader\n",
        "\n",
        "6. Then test it out on random example"
      ],
      "metadata": {
        "id": "owL8DylWMibn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading all the packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "import torch\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "h1Ir0LAvM7DR"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4eJpzCoX-Mc",
        "outputId": "99d6f091-1654-429c-e4f3-2e9d416652dd"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install kaggle\n",
        "!pip install kaggle\n",
        "\n",
        "# Upload your kaggle.json file (API key)\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "# Make a directory named kaggle and copy kaggle.json file there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d abhi8923shriv/sentiment-analysis-dataset\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip sentiment-analysis-dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "kw4_lWhJXzvT",
        "outputId": "8c823166-05b0-4ab1-a499-7de62aad1f43"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8b6100e0-60a1-409d-8eed-cb24375ddd36\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8b6100e0-60a1-409d-8eed-cb24375ddd36\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
            "Dataset URL: https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading sentiment-analysis-dataset.zip to /content\n",
            " 97% 53.0M/54.4M [00:03<00:00, 27.6MB/s]\n",
            "100% 54.4M/54.4M [00:03<00:00, 14.8MB/s]\n",
            "Archive:  sentiment-analysis-dataset.zip\n",
            "  inflating: test.csv                \n",
            "  inflating: testdata.manual.2009.06.14.csv  \n",
            "  inflating: train.csv               \n",
            "  inflating: training.1600000.processed.noemoticon.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'test.csv' is the correct filename after unzipping.\n",
        "# Adjust the path if necessary.\n",
        "try:\n",
        "  df_test = pd.read_csv('test.csv', encoding='latin-1')\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: 'test.csv' not found. Check the dataset extraction.\")"
      ],
      "metadata": {
        "id": "vxKnILTgQCnA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  df_train = pd.read_csv('train.csv', encoding='latin-1')\n",
        "except FileNotFoundError:\n",
        "  print(\"Error: 'test.csv' not found. Check the dataset extraction.\")"
      ],
      "metadata": {
        "id": "2oZl5nYPakWE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115
        },
        "id": "TG5ujbZGaBCP",
        "outputId": "0e0ebd0d-9ccb-4eb9-f2b7-86f1a5e3e595"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       textID                                               text sentiment  \\\n",
              "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral   \n",
              "\n",
              "  Time of Tweet Age of User      Country  Population -2020  Land Area (Km²)  \\\n",
              "0       morning        0-20  Afghanistan        38928346.0         652860.0   \n",
              "\n",
              "   Density (P/Km²)  \n",
              "0             60.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0d096b7-cd7c-4b1a-b4ba-0328f38d4f96\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>textID</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>Time of Tweet</th>\n",
              "      <th>Age of User</th>\n",
              "      <th>Country</th>\n",
              "      <th>Population -2020</th>\n",
              "      <th>Land Area (Km²)</th>\n",
              "      <th>Density (P/Km²)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>f87dea47db</td>\n",
              "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
              "      <td>neutral</td>\n",
              "      <td>morning</td>\n",
              "      <td>0-20</td>\n",
              "      <td>Afghanistan</td>\n",
              "      <td>38928346.0</td>\n",
              "      <td>652860.0</td>\n",
              "      <td>60.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0d096b7-cd7c-4b1a-b4ba-0328f38d4f96')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b0d096b7-cd7c-4b1a-b4ba-0328f38d4f96 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b0d096b7-cd7c-4b1a-b4ba-0328f38d4f96');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_test",
              "summary": "{\n  \"name\": \"df_test\",\n  \"rows\": 4815,\n  \"fields\": [\n    {\n      \"column\": \"textID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3534,\n        \"samples\": [\n          \"142108215\",\n          \"fb08563a7b\",\n          \"9a2c6ae21c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3534,\n        \"samples\": [\n          \" Thank you so much phaoloo !!!!\",\n          \"Midnight ice-cream weather! So **** bored\",\n          \"Ohh i forgot to tell you last night that when i was a alton towers i touched a shark  it was amazing !!!! it was nt a massive one tho\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"neutral\",\n          \"positive\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Time of Tweet\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"morning\",\n          \"noon\",\n          \"night\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age of User\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"0-20\",\n          \"21-30\",\n          \"70-100\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 195,\n        \"samples\": [\n          \"Philippines\",\n          \"Belgium\",\n          \"Sierra Leone\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Population -2020\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 146875664.36244348,\n        \"min\": 801.0,\n        \"max\": 1439323776.0,\n        \"num_unique_values\": 195,\n        \"samples\": [\n          109581078.0,\n          11589623.0,\n          7976983.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Land Area (Km\\u00b2)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1839133.911427382,\n        \"min\": 0.0,\n        \"max\": 16376870.0,\n        \"num_unique_values\": 193,\n        \"samples\": [\n          2267050.0,\n          1280000.0,\n          100250.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Density (P/Km\\u00b2)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1967.012367010644,\n        \"min\": 2.0,\n        \"max\": 26337.0,\n        \"num_unique_values\": 136,\n        \"samples\": [\n          400.0,\n          71.0,\n          331.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaJAL3tsafkO",
        "outputId": "601e1414-75fa-4682-cf7b-09e2fae66ef1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4815, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvx9DWcdapSw",
        "outputId": "d644938a-13ef-48e4-9dad-4bf3da85039d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27481, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0aHhrR0cOgg",
        "outputId": "3da4627b-2c24-46ca-a45b-4dce174c57d7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['textID', 'text', 'selected_text', 'sentiment', 'Time of Tweet',\n",
              "       'Age of User', 'Country', 'Population -2020', 'Land Area (Km²)',\n",
              "       'Density (P/Km²)'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Understanding"
      ],
      "metadata": {
        "id": "dGM5MOGJdB0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zI1o3Kfnbn20",
        "outputId": "674a4bb3-d46b-48d0-b080-836da519a9de"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27481 entries, 0 to 27480\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   textID            27481 non-null  object \n",
            " 1   text              27480 non-null  object \n",
            " 2   selected_text     27480 non-null  object \n",
            " 3   sentiment         27481 non-null  object \n",
            " 4   Time of Tweet     27481 non-null  object \n",
            " 5   Age of User       27481 non-null  object \n",
            " 6   Country           27481 non-null  object \n",
            " 7   Population -2020  27481 non-null  int64  \n",
            " 8   Land Area (Km²)   27481 non-null  float64\n",
            " 9   Density (P/Km²)   27481 non-null  int64  \n",
            "dtypes: float64(1), int64(2), object(7)\n",
            "memory usage: 2.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train[['text', 'sentiment']]"
      ],
      "metadata": {
        "id": "UMNsnZPRb5yb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncabhI-RcFvJ",
        "outputId": "a095592e-50e5-435a-8fe0-c503a2915953"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['text', 'sentiment'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8frFgEtudH4q",
        "outputId": "11d5c024-71f4-4f63-a569-2bb269f498ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 27481 entries, 0 to 27480\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   text       27480 non-null  object\n",
            " 1   sentiment  27481 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 429.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['sentiment'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DoGfMLHdVF6",
        "outputId": "1dc91d98-600a-4758-e817-b7f8adb4d7d0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neutral', 'negative', 'positive'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifies dominant categories.\n",
        "# Useful for data imbalance detection.\n",
        "df_train['sentiment'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "NWIyVoWKdcet",
        "outputId": "524841bc-ae61-486e-c9ec-a21f8a5575b0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sentiment\n",
              "neutral     11118\n",
              "positive     8582\n",
              "negative     7781\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>neutral</th>\n",
              "      <td>11118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>positive</th>\n",
              "      <td>8582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>negative</th>\n",
              "      <td>7781</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for the missing values\n",
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "SVFT9NhBd0Zv",
        "outputId": "5671eff1-cf15-44e8-b0ea-21b0af3172ea"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         1\n",
              "sentiment    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "Qtz6951vcUjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Missing values\n",
        "df_train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "Mv_vDnRAcXF0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "NYhq1aZTe5yz",
        "outputId": "e4234899-2794-4e03-ea36-50108de2d3ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text         0\n",
              "sentiment    0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentiment</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the natural launage (text column)"
      ],
      "metadata": {
        "id": "TVQeugpbfTKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['text'][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fdp1ewNTfBkK",
        "outputId": "9e24f2b9-0865-4f35-f54d-ced6b3f4cfac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my boss is bullying me...'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning\n",
        "1.   Remove Unwanted Characters\n",
        "2.   Lowercasing\n",
        "3.   Remove numbers\n",
        "\n",
        "# Tokenization\n",
        "1.   Covert text into small units, tokens (words or subwords)\n",
        "\n",
        "# Stop Word\n",
        "1.   Remove stop words such as \"the\", \"is\", \"in\" and others\n",
        "\n",
        "# Stemming\n",
        "1.   Reduce the word to root form. For example, \"running\", \"runner\" to \"run\"\n",
        "\n",
        "# Lemmatization\n",
        "1.   Reducing to root word with a proper meaning.\n",
        "\n",
        "# Removing Rare and Common Words\n",
        "1.   Rare - Word that appear too infrequently in the dataset.\n",
        "2.   Common - Word that appear too frequently in the dataset.\n",
        "\n",
        "# Handling Misspelling\n",
        "1.   Spell correcction\n",
        "\n",
        "# Part-of-Speech Tagging\n",
        "1.   Helps to understand the role of a word in a sentence.\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "1.   NER identifies proper nouns such as names of people, places, and organizations.\n",
        "\n",
        "# Vectorization (Feature Extraction)\n",
        "In order for the machine learning model to understand text, the words need to be represented as numerical data:\n",
        "\n",
        "1.   Bag of Words (BoW): Represents text as a vector based on the count of words.\n",
        "2.   TF-IDF (Term Frequency - Inverse Document Frequency): Adjusts word counts based on how common a word is across all documents.\n",
        "3.   Word Embeddings: Represent words as dense vectors (e.g., using Word2Vec, GloVe, or FastText).\n",
        "\n",
        "# Text Representation (Advanced Techniques)\n",
        "1.   Word2Vec: Uses neural networks to convert words into vector representations.\n",
        "2.   GloVe: Pre-trained word embeddings that represent words in a continuous vector space."
      ],
      "metadata": {
        "id": "eO92elOxgSt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_Cleaning(df, column, new_column):\n",
        "    \"\"\"\n",
        "    Cleans text data in the specified column of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The input DataFrame\n",
        "    column (str): The column name to clean\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with cleaned text\n",
        "\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess(text):\n",
        "\n",
        "      if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "      text = text.lower()\n",
        "      text = re.sub(r\"[^\\w\\s]\", \"\", text) # Removes punctuation\n",
        "      text = re.sub(r\"\\d+\", \"\", text)     # Remove numbers\n",
        "\n",
        "      tokens = word_tokenize(text)\n",
        "      tokens = [word for word in tokens if word not in stop_words]\n",
        "      tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "      return \" \".join(tokens)\n",
        "\n",
        "    df[new_column] = df[column].astype(str).apply(preprocess)\n",
        "    return df"
      ],
      "metadata": {
        "id": "Iwgm1HrRb92D"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = text_Cleaning(df_train, \"text\", \"cleaned_text\")"
      ],
      "metadata": {
        "id": "nFuCPIHbmVrw"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"cleaned_text\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CRwOSimHnSx_",
        "outputId": "715c2dd9-8189-435b-8791-0efe81312eac"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sooo sad miss san diego'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jACvDB9inhwj",
        "outputId": "348e5889-ecac-4b26-b211-38f07c5f23c4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sooo SAD I will miss you here in San Diego!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lnSMw_8Rnpzc",
        "outputId": "480b107c-abbf-4581-ff1a-49df99b0e6c3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I`d have responded, if I were going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"cleaned_text\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ck7E0tk3nlkA",
        "outputId": "45fd5c47-8d5d-4c27-9dc3-f77b2adbb343"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'id responded going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "\n",
        "# Load SpaCy model for Named Entity Recognition (NER)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def clean_text(df, column, new_column, rare_threshold=2, common_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Cleans text data in the specified column and stores it in a new column.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The input DataFrame\n",
        "    column (str): The column name containing raw text\n",
        "    new_column (str): The column name to store cleaned text\n",
        "    rare_threshold (int): The minimum frequency for rare words (default: 2)\n",
        "    common_threshold (float): The fraction threshold for common words (default: 50%)\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with the new cleaned text column\n",
        "    \"\"\"\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Compute word frequencies\n",
        "    all_words = [word for text in df[column].dropna() for word in word_tokenize(str(text).lower())]\n",
        "    word_freq = Counter(all_words)\n",
        "\n",
        "    # Define rare and common words\n",
        "    rare_words = {word for word, freq in word_freq.items() if freq <= rare_threshold}\n",
        "    common_words = {word for word, freq in word_freq.items() if freq >= common_threshold * len(df)}\n",
        "\n",
        "    def preprocess(text):\n",
        "        if pd.isna(text):  # Handle missing values\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "\n",
        "        # Remove punctuation and unwanted characters\n",
        "        text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "\n",
        "        # Remove numbers\n",
        "        text = re.sub(r\"\\d+\", \"\", text)\n",
        "\n",
        "        # Tokenization\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Stopword Removal\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "        # Stemming\n",
        "        stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "        # Lemmatization\n",
        "        lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "        # Remove Rare and Common Words\n",
        "        tokens = [word for word in lemmatized_tokens if word not in rare_words and word not in common_words]\n",
        "\n",
        "        # Spell Correction\n",
        "        corrected_tokens = [TextBlob(word).correct() for word in tokens]\n",
        "\n",
        "        # POS Tagging\n",
        "        pos_tags = nltk.pos_tag(corrected_tokens)\n",
        "\n",
        "        # Named Entity Recognition (NER)\n",
        "        doc = nlp(\" \".join(corrected_tokens))\n",
        "        named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "        return {\n",
        "            \"cleaned_text\": \" \".join(corrected_tokens),\n",
        "            \"pos_tags\": pos_tags,\n",
        "            \"named_entities\": named_entities\n",
        "        }\n",
        "\n",
        "    # Apply preprocessing and extract data\n",
        "    processed_data = df[column].astype(str).apply(preprocess)\n",
        "\n",
        "    # Store cleaned text, POS tags, and NER results in new columns\n",
        "    df[new_column] = processed_data.apply(lambda x: x[\"cleaned_text\"])\n",
        "    df[\"pos_tags\"] = processed_data.apply(lambda x: x[\"pos_tags\"])\n",
        "    df[\"named_entities\"] = processed_data.apply(lambda x: x[\"named_entities\"])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8fjLxuqooOM",
        "outputId": "aa53285d-70ad-418d-b40c-949d6e4fc171"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    }
  ]
}